{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Complex processing and data pipelines.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvfQ2jSZNawW",
        "colab_type": "text"
      },
      "source": [
        "## Quick pipeline\n",
        "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
        "\n",
        "The spark context is defined, along with the pyspark.sql.functions library being aliased as F as is customary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzbTCXVENgI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the data to a DataFrame\n",
        "departures_df = spark.read.csv(\"2015-departures.csv.gz\", header=True)\n",
        "\n",
        "# Remove any duration of 0\n",
        "departures_df = departures_df.filter(departures_df[3]>0)\n",
        "\n",
        "# Add an ID column\n",
        "departures_df = departures_df.withColumn('id',F.monotonically_increasing_id() )\n",
        "\n",
        "# Write the file out to JSON format\n",
        "departures_df.write.json('output.json', mode='overwrite')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO8sIU_kNvc-",
        "colab_type": "text"
      },
      "source": [
        "## Pipeline data issue\n",
        "After creating your quick pipeline, you provide the json file to an analyst on your team. After loading the data and performing a couple exploratory tasks, the analyst tells you there's a problem in the dataset while trying to sort the duration data. She's not sure what the issue is beyond the sorting operation not working as expected.\n",
        "\n",
        "Date  ||        Flight|| Number ||  Airport     Duration  ||  ID\n",
        "\n",
        "09/30/2015||    2287 ||           ANC   \n",
        "\n",
        "409       ||  107962||\n",
        "12/28/2015||    1408  ||          OKC     \n",
        "\n",
        "41   ||  141917\n",
        "\n",
        "08/11/2015 ||   2287   ||         ANC   \n",
        "\n",
        "410         || 87978||\n",
        "\n",
        "After analyzing the data, which command would fix the issue?\n",
        "\n",
        "Possible Answers\n",
        "departures_df = departures_df.orderBy(departures_df.Airport)\n",
        "\n",
        "departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))    **********\n",
        "\n",
        "departures_df = departures_df.orderBy(departures_df['Duration'])\n",
        "\n",
        "departures_df = departures_df.select(departures_df['Duration']).cast(LongType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj32TAmF0L0O",
        "colab_type": "text"
      },
      "source": [
        "## Removing commented lines\n",
        "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
        "\n",
        "To start, you need to remove all commented rows in the dataset.\n",
        "\n",
        "The spark context, and the base CSV file (annotations.csv.gz) are available for you to work with. The col function is also available for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZWHevLj0PzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the file to a DataFrame and perform a row count\n",
        "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
        "full_count = annotations_df.count()\n",
        "annotations_df.show()\n",
        "# Count the number of rows beginning with '#'\n",
        "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
        "\n",
        "# Import the file to a new DataFrame, without commented rows\n",
        "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
        "\n",
        "# Count the new DataFrame and verify the difference is as expected\n",
        "no_comments_count = no_comments_df.count()\n",
        "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwWgEq8l2RY7",
        "colab_type": "text"
      },
      "source": [
        "## Removing invalid rows\n",
        "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (\\t) characters.\n",
        "\n",
        "The DataFrame annotations_df is already available, with the commented rows removed. The spark.sql.functions library is available under the alias F. The initial number of rows available in the DataFrame is stored in the variable initial_count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_3wPrZh2TcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split _c0 on the tab character and store the list in a variable\n",
        "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
        "\n",
        "# Create the colcount column on the DataFrame\n",
        "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
        "\n",
        "# Remove any rows containing fewer than 5 fields\n",
        "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\n",
        "\n",
        "# Count the number of rows\n",
        "final_count = annotations_df_filtered.count()\n",
        "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbb97WCS4AHy",
        "colab_type": "text"
      },
      "source": [
        "## Splitting into columns\n",
        "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content.\n",
        "\n",
        "You have the spark context and the latest version of the annotations_df DataFrame. pyspark.sql.functions is available under the alias F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boo1EhuK4CNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the content of _c0 on the tab character (aka, '\\t')\n",
        "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
        "\n",
        "# Add the columns folder, filename, width, and height\n",
        "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
        "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
        "split_df = split_df.withColumn('width',split_cols.getItem(2))\n",
        "split_df = split_df.withColumn('height',split_cols.getItem(3))\n",
        "\n",
        "# Add split_cols as a column\n",
        "split_df = split_df.withColumn(\"split_cols\",split_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v99vXZRd4Iw2",
        "colab_type": "text"
      },
      "source": [
        "## Further parsing\n",
        "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns.\n",
        "\n",
        "The spark context is available and pyspark.sql.functions is aliased as F. The types from pyspark.sql.types are already imported. The split_df DataFrame is as you last left it. Remember, you can use .printSchema() on a DataFrame in the console area to view the column names and types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELYbQcPZ19fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retriever(cols, colcount):\n",
        "  # Return a list of dog data\n",
        "  return cols[4:colcount]\n",
        "\n",
        "# Define the method as a UDF\n",
        "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
        "\n",
        "# Create a new column using your UDF\n",
        "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
        "\n",
        "# Remove the original column, split_cols, and the colcount\n",
        "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_uqiDyg2DHA",
        "colab_type": "text"
      },
      "source": [
        "## Validate rows via join\n",
        "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named valid_folders_df. The DataFrame split_df is as you last left it with a group of split columns.\n",
        "\n",
        "The spark object is available, and pyspark.sql.functions is imported as F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s557S0Sh2Ep8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename the column in valid_folders_df\n",
        "valid_folders_df = valid_folders_df.withColumnRenamed('_c0','folder')\n",
        "\n",
        "# Count the number of rows in split_df\n",
        "split_count = split_df.count()\n",
        "# Join the DataFrames\n",
        "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
        "\n",
        "# Compare the number of rows remaining\n",
        "joined_count = joined_df.count()\n",
        "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMc112k92HQ_",
        "colab_type": "text"
      },
      "source": [
        "## Examining invalid rows\n",
        "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
        "\n",
        "You want to find the difference between two DataFrames and store the invalid rows.\n",
        "\n",
        "The spark object is defined and pyspark.sql.functions are imported as F. The original DataFrame split_df and the joined DataFrame joined_df are available as they were in their previous states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blr6i2Xo2K_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine the row counts for each DataFrame\n",
        "split_count = split_df.count()\n",
        "joined_count = joined_df.count()\n",
        "\n",
        "# Create a DataFrame containing the invalid rows\n",
        "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
        "\n",
        "# Validate the count of the new DataFrame is as expected\n",
        "invalid_count = invalid_df.count()\n",
        "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
        "\n",
        "# Determine the number of distinct folder rows removed\n",
        "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
        "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbgEYUFQi9_",
        "colab_type": "text"
      },
      "source": [
        "## Dog parsing\n",
        "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.\n",
        "\n",
        "The joined_df DataFrame is as you last defined it, and the pyspark.sql.types have all been imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGFKfdiudpwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select the dog details and show 10 untruncated rows\n",
        "print(joined_df.select('dog_list').show(10, truncate=False))\n",
        "\n",
        "# Define a schema type for the details in the dog list\n",
        "DogType = StructType([\n",
        "\tStructField(\"breed\", StringType(), False),\n",
        "    StructField(\"start_x\", IntegerType(), False),\n",
        "    StructField(\"start_y\", IntegerType(), False),\n",
        "    StructField(\"end_x\", IntegerType(), False),\n",
        "    StructField(\"end_y\", IntegerType(), False)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrhHxYb0dqgS",
        "colab_type": "text"
      },
      "source": [
        "## Per image count\n",
        "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your dog_list column created earlier. You have also created the DogType which will allow better parsing of the data within some of the data columns.\n",
        "\n",
        "The joined_df is available as you last defined it, and the DogType StructType is defined. pyspark.sql.functions is available under the F alias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMyVuJvDdwEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a function to return the number and type of dogs as a tuple\n",
        "def dogParse(doglist):\n",
        "  dogs = []\n",
        "  for dog in doglist:\n",
        "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
        "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
        "  return dogs\n",
        "\n",
        "# Create a UDF\n",
        "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
        "\n",
        "# Use the UDF to list of dogs and drop the old column\n",
        "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
        "\n",
        "# Show the number of dogs in the first 10 rows\n",
        "joined_df.select(F.size('dogs')).show(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXpJr_Mqd102",
        "colab_type": "text"
      },
      "source": [
        "## Percentage dog pixels\n",
        "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
        "\n",
        "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
        "\n",
        "(Xend - Xstart) * (Yend - Ystart)\n",
        "\n",
        "NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
        "\n",
        "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100.\n",
        "The joined_df DataFrame is as you last used it. pyspark.sql.functions is aliased to F."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhDOF6Nhd4QQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FawD3ZaBd6S3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a UDF to determine the number of pixels per image\n",
        "def dogPixelCount(doglist):\n",
        "  totalpixels = 0\n",
        "  for dog in doglist:\n",
        "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
        "  return totalpixels\n",
        "\n",
        "# Define a UDF for the pixel count\n",
        "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
        "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
        "\n",
        "# Create a column representing the percentage of pixels\n",
        "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
        "\n",
        "# Show the first 10 annotations with more than 60% dog\n",
        "joined_df.where('dog_percent > 60').show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}