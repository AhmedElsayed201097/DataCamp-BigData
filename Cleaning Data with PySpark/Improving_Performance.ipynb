{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improving Performance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iek0nLjeYfL",
        "colab_type": "text"
      },
      "source": [
        "## Caching a DataFrame\n",
        "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
        "\n",
        "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
        "\n",
        "The DataFrame departures_df is defined, but no actions have been performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcuuuHTGeatA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Add caching to the unique rows in departures_df\n",
        "departures_df = departures_df.distinct().cache()\n",
        "\n",
        "# Count the unique rows in departures_df, noting how long the operation takes\n",
        "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
        "\n",
        "# Count the rows again, noting the variance in time of a cached DataFrame\n",
        "start_time = time.time()\n",
        "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV6Whbfdecr1",
        "colab_type": "text"
      },
      "source": [
        "## Removing a DataFrame from cache\n",
        "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
        "\n",
        "The DataFrame departures_df is defined and has already been cached for you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIW4Kd9mfS9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine if departures_df is in the cache\n",
        "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
        "print(\"Removing departures_df from cache\")\n",
        "\n",
        "# Remove departures_df from the cache\n",
        "departures_df.unpersist()\n",
        "\n",
        "# Check the cache status again\n",
        "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7vzhMg1fc5S",
        "colab_type": "text"
      },
      "source": [
        "## File import performance\n",
        "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
        "\n",
        "You have two types of files available: departures_full.txt.gz and departures_xxx.txt.gz where xxx is 000 - 013. The same number of rows is split between each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TpbilBVfePq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the full and split files into DataFrames\n",
        "full_df = spark.read.csv('departures_full.txt.gz')\n",
        "split_df = spark.read.csv('departures_*.txt.gz')\n",
        "\n",
        "# Print the count and run time for each DataFrame\n",
        "start_time_a = time.time()\n",
        "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
        "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
        "\n",
        "start_time_b = time.time()\n",
        "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
        "print(\"Time to run: %f\" % (time.time() - start_time_b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANKA-1Y6ff__",
        "colab_type": "text"
      },
      "source": [
        "## Reading Spark configurations\n",
        "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
        "\n",
        "The spark object is available for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN9uQRGMfk1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Name of the Spark application instance\n",
        "app_name = spark.conf.get('spark.app.name')\n",
        "\n",
        "# Driver TCP port\n",
        "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
        "\n",
        "# Number of join partitions\n",
        "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
        "\n",
        "# Show the results\n",
        "print(\"Name: %s\" % app_name)\n",
        "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
        "print(\"Number of partitions: %s\" % num_partitions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECtP2Mj7fnJu",
        "colab_type": "text"
      },
      "source": [
        "## Writing Spark configurations\n",
        "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
        "\n",
        "The spark configuration is initially set to the default value of 200 partitions.\n",
        "\n",
        "The spark object is available for use. A file named departures.txt.gz is available for import. An initial DataFrame containing the distinct rows from departures.txt.gz is available as departures_df."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VrbB16afsAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store the number of partitions in variable\n",
        "before = departures_df.rdd.getNumPartitions()\n",
        "\n",
        "# Configure Spark to use 500 partitions\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
        "\n",
        "# Recreate the DataFrame using the departures data file\n",
        "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
        "\n",
        "# Print the number of partitions for each instance\n",
        "print(\"Partition count before change: %d\" % before)\n",
        "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxZyDDqffu4u",
        "colab_type": "text"
      },
      "source": [
        "**Normal** **joins**\n",
        "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan.\n",
        "\n",
        "The DataFrames flights_df and airports_df are available to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_iskV09f0HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join the flights_df and aiports_df DataFrames\n",
        "normal_df = flights_df.join(airports_df, \\\n",
        "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
        "\n",
        "# Show the query plan\n",
        "normal_df.explain()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ahS9ENZf76Q",
        "colab_type": "text"
      },
      "source": [
        "## Using broadcasting on Spark joins\n",
        "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
        "\n",
        "A couple tips:\n",
        "\n",
        "Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
        "On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
        "If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.\n",
        "The DataFrames flights_df and airports_df are available to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZcKK63Xf-NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the broadcast method from pyspark.sql.functions\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Join the flights_df and airports_df DataFrames using broadcasting\n",
        "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
        "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
        "\n",
        "# Show the query plan and compare against the original\n",
        "broadcast_df.explain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWCSop6gAhQ",
        "colab_type": "text"
      },
      "source": [
        "## Comparing broadcast vs normal joins\n",
        "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
        "\n",
        "Your DataFrames normal_df and broadcast_df are available for your use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rI71dQMgEkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "# Count the number of rows in the normal DataFrame\n",
        "normal_count = normal_df.count()\n",
        "normal_duration = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "# Count the number of rows in the broadcast DataFrame\n",
        "broadcast_count = broadcast_df.count()\n",
        "broadcast_duration = time.time() - start_time\n",
        "\n",
        "# Print the counts and the duration of the tests\n",
        "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
        "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}