{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark MLlib algorithms\n",
    "Before using any Machine learning algorithms in PySpark shell, you'll have to import the submodules of pyspark.mllib library and then choose the appropriate class that is needed for a specific machine learning task.\n",
    "\n",
    "In this simple exercise, you'll learn how to import the different submodules of pyspark.mllib along with the classes that are needed for performing Collaborative filtering, Classification and Clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Movie Lens dataset into RDDs\n",
    "Collaborative filtering is a technique for recommender systems wherein users' ratings and interactions with various products are used to recommend new ones. With the advent of Machine Learning and parallelized processing of data, Recommender systems have become widely popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags. In this 3-part exercise, your goal is to develop a simple movie recommendation system using PySpark MLlib using a subset of MovieLens 100k dataset.\n",
    "\n",
    "In the first part, you'll first load the MovieLens data (ratings.csv) into RDD and from each line in the RDD which is formatted as userId,movieId,rating,timestamp, you'll need to map the MovieLens data to a Ratings object (userID, productID, rating) after removing timestamp column and finally you'll split the RDD into training and test RDDs.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path variable (which is the path to the ratings.csv file), and ALS class are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS,Rating\n",
    "# Load the data into RDD\n",
    "data=sc.textFile(\"ratings.csv\")\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=31, rating=2.5),\n",
       " Rating(user=1, product=1172, rating=4.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and predictions\n",
    "After splitting the data into training and test data, in the second part of the exercise, you'll train the ALS algorithm using the training data. PySpark MLlib's ALS algorithm has the following mandatory parameters - rank (the number of latent factors in the model) and iterations (number of iterations to run). After training the ALS model, you can use the model to predict the ratings from the test data. For this, you will provide the user and item columns from the test dataset and finally print the first 2 rows of predictAll() output.\n",
    "\n",
    "Remember, you have SparkContext sc, training_data and test_data are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=336, product=1084, rating=3.473091302303917),\n",
       " Rating(user=232, product=1084, rating=4.01293731622536)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation using MSE\n",
    "After generating the predicted ratings from the test data using ALS model, in this final part of the exercise, you'll prepare the data for calculating Mean Square Error (MSE) of the model. The MSE is the average value of (original rating – predicted rating)^2 for all users and indicates the absolute fit of the model to the data. To do this, first, you'll organize both the ratings and prediction RDDs to make a tuple of ((user, product), rating)), then join the ratings RDD with prediction RDD and finally apply a squared difference function along with mean() to get the MSE.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also, ratings_final and predictions RDD are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the model for the test data = 1.35\n"
     ]
    }
   ],
   "source": [
    "# Prepare ratings data\n",
    "rates = test_data.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "\n",
    "\n",
    "# Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "denseVect=Vectors.sparse(3,{1:0,3:4,5:5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {1: 0.0, 3: 4.0, 5: 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denseVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp=LabeledPoint(1,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(145, {0: 1.0, 6: 1.0, 37: 1.0, 41: 1.0, 52: 1.0, 80: 2.0, 114: 1.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "sentece=\" hello hello world do iu ahmed sayrf\"\n",
    "words=sentece.split(\" \")\n",
    "tf=HashingTF(145)\n",
    "tf.transform(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading spam and non-spam data\n",
    "Logistic Regression is a popular method to predict a categorical response. Probably one of the most common applications of the logistic regression is the message or email spam classification. In this 3-part exercise, you'll create an email spam classifier with logistic regression using Spark MLlib. Here are the brief steps for creating a spam classifier.\n",
    "\n",
    "1- Create an RDD of strings representing email.\n",
    "\n",
    "2-Run MLlib’s feature extraction algorithms to convert text into an RDD of vectors.\n",
    "\n",
    "3-Call a classification algorithm on the RDD of vectors to return a model object to classify new points.\n",
    "\n",
    "4-Evaluate the model on a test dataset using one of MLlib’s evaluation functions.\n",
    "\n",
    "In the first part of the exercise, you'll load the 'spam' and 'ham' (non-spam) files into RDDs, split the emails into individual words and look at the first element in each of the RDD.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path_spam variable (which is the path to the 'spam' file) and file_path_non_spam (which is the path to the 'non-spam' file) is already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element in spam_words is You\n",
      "The first element in non_spam_words is Rofl.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets into RDDs\n",
    "spam_rdd = sc.textFile(\"spam.txt\")\n",
    "non_spam_rdd = sc.textFile(\"non_spam.txt\")\n",
    "\n",
    "# Split the email messages into words\n",
    "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# Print the first element in the split RDD\n",
    "print(\"The first element in spam_words is\", spam_words.first())\n",
    "print(\"The first element in non_spam_words is\", non_spam_words.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature hashing and LabelPoint\n",
    "After splitting the emails into words, our raw data set of 'spam' and 'non-spam' is currently composed of 1-line messages consisting of spam and non-spam messages. In order to classify these messages, we need to convert text into features.\n",
    "\n",
    "In the second part of the exercise, you'll first create a HashingTF() instance to map text to vectors of 200 features, then for each message in 'spam' and 'non-spam' files you'll split them into words, and each word is mapped to one feature. These are the features that will be used to decide whether a message is 'spam' or 'non-spam'. Next, you'll create labels for features. For a valid message, the label will be 0 (i.e. the message is not spam) and for a 'spam' message, the label will be 1 (i.e. the message is spam). Finally, you'll combine both the labeled datasets.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also spam_words and non_spam_words variables are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HashingTf instance with 200 features\n",
    "tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# Map each word to one feature\n",
    "spam_features = tf.transform(spam_words)\n",
    "non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# Label the features: 1 for spam, 0 for non-spam\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# Combine the two datasets\n",
    "samples = spam_samples.join(non_spam_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model training\n",
    "After creating labels and features for the data, we’re ready to build a model that can learn from it (training). But before you train the model, you'll split the combined dataset into training and testing dataset because it can assign a probability of being spam to each data point. We can then decide to classify messages as spam or not, depending on how high the probability.\n",
    "\n",
    "In this final part of the exercise, you'll split the data into training and test, run Logistic Regression on the training data, apply the same HashingTF() feature transformation to get vectors on a positive example (spam) and a negative one (non-spam) and finally check the accuracy of the model trained.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace, as well as the samples variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 562.0 failed 1 times, most recent failure: Lost task 3.0 in stage 562.0 (TID 600, DESKTOP-PKOJNM3, executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:609)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:280)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:373)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:213)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor125.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:609)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:280)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:373)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:213)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-2fbac23b3a52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Create a prediction label from the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitialWeights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnumClasses\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0minitialWeights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m         \"\"\"\n\u001b[1;32m-> 1451\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 562.0 failed 1 times, most recent failure: Lost task 3.0 in stage 562.0 (TID 600, DESKTOP-PKOJNM3, executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:609)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:280)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:373)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:213)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor125.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:609)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:280)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:373)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:213)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Split the data into training and testing\n",
    "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# Create a prediction label from the test data\n",
    "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# Combine original labels with the predicted labels\n",
    "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "\n",
    "# Check the accuracy of the model on the test data\n",
    "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and parsing the 5000 points data\n",
    "Clustering is the unsupervised learning task that involves grouping objects into clusters of high similarity. Unlike the supervised tasks, where data is labeled, clustering can be used to make sense of unlabeled data. PySpark MLlib includes the popular K-means algorithm for clustering. In this 3 part exercise, you'll find out how many clusters are there in a dataset containing 5000 rows and 2 columns. For this you'll first load the data into an RDD, parse the RDD based on the delimiter, run the KMeans model, evaluate the model and finally visualize the clusters.\n",
    "\n",
    "In the first part, you'll load the data into RDD, parse the RDD based on the delimiter and convert the string type of the data to an integer.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path variable (which is the path to the 5000_points.txt file) is already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 rows in the rdd_split_int dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a RDD\n",
    "clusterRDD = sc.textFile(\"wine.txt\")\n",
    "\n",
    "# Split the RDD based on tab\n",
    "rdd_split = clusterRDD.map(lambda x: x.split('\\t'))\n",
    "\n",
    "# Transform the split RDD by creating a list of integers\n",
    "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# Count the number of rows in RDD \n",
    "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means training\n",
    "Now that the RDD is ready for training, in the second part of the exercise, you'll train the RDD with PySpark's MLlib's KMeans algorithm. The algorithm is somewhat naive--it clusters the data into k clusters, even if k is not the right number of clusters to use. Therefore, when using k-means clustering, the most important parameter is a target number of clusters to generate, k. In practice, you rarely know the “true” number of clusters in advance, so the best practice is to try several values of k until the average intracluster distance stops decreasing dramatically\n",
    "\n",
    "In this 2nd part, you'll test with k's ranging from 13 to 16 and use the elbow method to chose the correct k. The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k, calculate Within Set Sum of Squared Error (WSSSE, this function is already provided to you) and select the best k based on the sudden drop in WSSSE. Finally, you'll retrain the model with the best k (15 in this case) and print the centroids (cluster centers).\n",
    "\n",
    "Remember, you already have a SparkContext sc and rdd_split_int RDD available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cluster 13 has Within Set Sum of Squared Error 252060331.1260039\n",
      "The cluster 14 has Within Set Sum of Squared Error 215808893.2796594\n",
      "The cluster 15 has Within Set Sum of Squared Error 169394691.52639425\n",
      "The cluster 16 has Within Set Sum of Squared Error 168103213.0429197\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from math import sqrt\n",
    "def error(point):\n",
    "    center = model.centers[model.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "# Train the model with clusters from 13 to 16 and compute WSSSE \n",
    "for clst in range(13, 17):\n",
    "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
    "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))\n",
    "\n",
    "# Train the model again with the best k \n",
    "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = model.clusterCenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing clusters\n",
    "After KMeans model training with an optimum K value (K = 15), in this final part of the exercise, you will visualize the clusters and their cluster centers (centroids) and see if they overlap with each other. For this, you'll first convert rdd_split_int RDD into spark DataFrame and then into Pandas DataFrame for plotting. Similarly, you'll convert cluster_centers into Pandas DataFrame. Once the DataFrames are created, you'll use matplotlib library to create scatter plots.\n",
    "\n",
    "Remember, you already have a SparkContext sc, rdd_split_int and cluster_centers variables available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Convert rdd_split_int RDD into Spark DataFrame\n",
    "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n",
    "\n",
    "# Convert Spark DataFrame into Pandas DataFrame\n",
    "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
    "\n",
    "# Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame\n",
    "cluster_centers_pandas = pd.DataFrame(model.clusterCenters, columns=[\"col1\", \"col2\"])\n",
    "\n",
    "# Create an overlaid scatter plot\n",
    "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
    "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
